{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "import io\n",
    "import numpy as np\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = '../datafiles'\n",
    "GLOVE_ZIP_FILE_URL = 'http://nlp.stanford.edu/data/glove.840B.300d.zip'\n",
    "GLOVE_ZIP_FILE = 'glove.840B.300d.zip'\n",
    "GLOVE_FILE = 'glove.840B.300d.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXTRACTION OF QUESSTION PAIR DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makedataset(filepath, delimiter, trainfile = False):\n",
    "    print('Preprocessing {} file'.format(filepath))\n",
    "    question1 = []\n",
    "    question2 = []\n",
    "    is_duplicate = []\n",
    "    with open(filepath, encoding='utf-8') as questionfile:\n",
    "        reader = csv.DictReader(questionfile, delimiter = delimiter)\n",
    "        for row in reader:\n",
    "            question1.append(row['question1'])\n",
    "            question2.append(row['question2'])\n",
    "            \n",
    "            if trainfile:\n",
    "                is_duplicate.append(row['is_duplicate'])\n",
    "    questionfile.close()\n",
    "    \n",
    "    print('Processed number of Questions : {}'.format(len(question1) + len(question2)))\n",
    "    return question1, question2, is_duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing ../datafiles\\train.csv file\n",
      "Processed number of Questions : 808580\n"
     ]
    }
   ],
   "source": [
    "question1, question2, is_duplicate = makedataset(os.path.join(DATASET_DIR, 'train.csv'), delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class vocab_builder():\n",
    "    def __init__(self, sentences):\n",
    "        self.word2ind = {}\n",
    "        self.ind2word = {}\n",
    "        self.word_bag = []\n",
    "        self.index = 0\n",
    "        self.build_vocab(sentences)\n",
    "    \n",
    "    def build_vocab(self, sentences):\n",
    "        tokens = []\n",
    "        for sentence in sentences:\n",
    "            words = nltk.tokenize.word_tokenize(sentence.lower())\n",
    "            tokens.extend(words)\n",
    "            \n",
    "        for word in tokens:\n",
    "            self.add_word(word)\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2ind:\n",
    "            self.word2ind[word] = self.index\n",
    "            self.ind2word[self.index] = word\n",
    "            self.index += 1\n",
    "            self.word_bag.append(word)\n",
    "            \n",
    "    def get_index(self, word):\n",
    "        if word in self.word2ind:\n",
    "            return self.word2ind[word]\n",
    "        else:\n",
    "            print('Word not in Vocabulary')\n",
    "    \n",
    "    def get_word(self, index):\n",
    "        return self.ind2word[index]\n",
    "    \n",
    "    def get_sentence(self, index_list):\n",
    "        sentence = ''\n",
    "        for curr_ind in index_list:\n",
    "            curr_word = self.get_word(curr_ind)\n",
    "            sentence += \" \" + curr_word\n",
    "        return sentence\n",
    "    \n",
    "    def get_tokenizers(self):\n",
    "        return self.word_bag\n",
    "    \n",
    "    def text_to_index(self, sentences):\n",
    "        sent2ind = []\n",
    "        for sent in sentences:\n",
    "            words = nltk.tokenize.word_tokenize(sent.lower())\n",
    "            curr_sent_index = [self.get_index(word) for word in words]\n",
    "            sent2ind.append(curr_sent_index)\n",
    "        return sent2ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building tokenized Word Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = question1 + question2\n",
    "vocab = vocab_builder(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "question1_word_tokenizer = vocab.text_to_index(question1)\n",
    "question2_word_tokenizer = vocab.text_to_index(question2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 3, 5, 6, 7, 8, 9, 10, 8, 11, 12]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question1_word_tokenizer[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(question1_word_tokenizer[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 13, 14, 15, 16, 17, 18, 19, 12]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question1_word_tokenizer[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(question1_word_tokenizer[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'what': 0, 'is': 1, 'the': 2, 'step': 3, 'by': 4}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(list(vocab.word2ind.items())[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading and Processing of GloVe Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gloveEmbedding(filepath, filename, fileURL=None):\n",
    "    if not os.path.exists(os.path.join(filepath, filename)):\n",
    "        if fileURL:\n",
    "            print('-' * 100)\n",
    "            print('File Not Exists!! Downloading it from the Server.....')\n",
    "            req = requests.get(fileURL)\n",
    "            print('File Downloaded! Extracting it .....')\n",
    "            zipcontent = ZipFile(io.BytesIO(req.content))\n",
    "            zipcontent.extractall()\n",
    "            print('-' * 100)\n",
    "        else:\n",
    "            print('No Path Specified')\n",
    "    print('Processing the GLoVE File .....')\n",
    "    print('-' * 100)\n",
    "    embeddings_index = {}\n",
    "    \n",
    "    with open(os.path.join(filepath, filename), encoding = 'utf-8') as encoding_file:\n",
    "        for line in encoding_file:\n",
    "            value = line.split(' ')\n",
    "            word = value[0]\n",
    "            embedding_val = np.asarray(value[1:], dtype = np.float32)\n",
    "            embeddings_index[word] = embedding_val\n",
    "    encoding_file.close()\n",
    "    print('Length of Word Embedding : {}'.format(len(embeddings_index)))\n",
    "    print('-' * 100)\n",
    "    return embeddings_index    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gloveEmbedding(os.path.join(DATASET_DIR, GLOVE_FILE), GLOVE_ZIP_FILE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
